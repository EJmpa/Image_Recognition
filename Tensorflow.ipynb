{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EJmpa/Image_Recognition/blob/main/Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "517c95e4-5254-4788-9dc6-2d082b158ce6",
      "metadata": {
        "id": "517c95e4-5254-4788-9dc6-2d082b158ce6"
      },
      "source": [
        "# 1. Importation of libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "257f0c2b-0b81-4178-ae9f-22fb562f320a",
      "metadata": {
        "id": "257f0c2b-0b81-4178-ae9f-22fb562f320a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Conv2D, Dense, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3357fb29-c7e9-4cab-aaab-c68a9aefd050",
      "metadata": {
        "id": "3357fb29-c7e9-4cab-aaab-c68a9aefd050"
      },
      "source": [
        "# 2. Loading MNIST Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2edaddd-8e9b-49c1-a7d0-1fc0e39a45f7",
      "metadata": {
        "id": "a2edaddd-8e9b-49c1-a7d0-1fc0e39a45f7"
      },
      "source": [
        "The `MNIST` Dataset contains `70,000 28x28` images showing handwritten digits.\n",
        "\n",
        "The function returns the dataset split into `train` and `test` sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17605394-ee3a-486a-bd16-3df78f8c11a8",
      "metadata": {
        "id": "17605394-ee3a-486a-bd16-3df78f8c11a8",
        "outputId": "b1d77253-1b7b-4212-f159-1328c942ac48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 4s 0us/step\n",
            "x_train shape: (60000, 28, 28)\n",
            "y_train shape: (60000,)\n",
            "x_test shape: (10000, 28, 28)\n",
            "y_test shape: (10000,)\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c29d8a86-d1b0-414a-bfc2-4470855ac912",
      "metadata": {
        "id": "c29d8a86-d1b0-414a-bfc2-4470855ac912"
      },
      "source": [
        "The `x_train` and `x_test` contain the `train` and `test` images.\n",
        "\n",
        "The `y_train` and `y_test` contain the target values: a number between `0` and `9` indicating the digit shown in the corresponding image.\n",
        "\n",
        "There are `60,000` images to train the model and `10,000` to test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30db1363-9cbc-4083-8121-148b8d6ba3e0",
      "metadata": {
        "id": "30db1363-9cbc-4083-8121-148b8d6ba3e0"
      },
      "source": [
        "When dealing with images, a tensor with 4 dimensions: `batch size`, `width`, `height`, and `color channels`.\n",
        "\n",
        "`x_train` is `(60 000, 28, 28)`. It must be reshaped to add missing dimension `(\"1\" because these images are grayscale)`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f37800e7-9e8f-4aa8-b29e-2e26fa71891e",
      "metadata": {
        "id": "f37800e7-9e8f-4aa8-b29e-2e26fa71891e"
      },
      "source": [
        "# 3. Reshape x_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81dc3f65-1581-40db-8e12-9325c1d1c4d1",
      "metadata": {
        "id": "81dc3f65-1581-40db-8e12-9325c1d1c4d1"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94b8920b-cfe2-4a12-a082-fff3cc07e9b6",
      "metadata": {
        "id": "94b8920b-cfe2-4a12-a082-fff3cc07e9b6"
      },
      "source": [
        "Each pixel goes from `0` to `255`. Neural networks work much better with smaller values.\n",
        "\n",
        "Here we normalize pixels by dividing them by `255`. That way, each pixel will go from `0` to `1`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28735cb7-a737-4888-a3b4-c2b4e983813c",
      "metadata": {
        "id": "28735cb7-a737-4888-a3b4-c2b4e983813c"
      },
      "source": [
        "## 3.1. Normalize x_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfd5988c-c5d1-4d61-adca-905abba078e7",
      "metadata": {
        "id": "cfd5988c-c5d1-4d61-adca-905abba078e7"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.astype('float32')/255"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ed224b1-9828-4bb1-894b-078299b5fe15",
      "metadata": {
        "id": "9ed224b1-9828-4bb1-894b-078299b5fe15"
      },
      "source": [
        "Target values go from `0` to `9` (the value of each digit.)\n",
        "\n",
        "This line one-hot encodes these values.\n",
        "\n",
        "For example, this will transform a value like 5 in an array of zeros with a single 1 corresponding to the fifth position:\n",
        "\n",
        "`[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dede97dd-c188-4890-a3dd-e7fc6690da28",
      "metadata": {
        "id": "dede97dd-c188-4890-a3dd-e7fc6690da28"
      },
      "outputs": [],
      "source": [
        "y_train = to_categorical(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2afbedb-b30f-4cf1-a52b-6c85ba733693",
      "metadata": {
        "id": "f2afbedb-b30f-4cf1-a52b-6c85ba733693"
      },
      "source": [
        "# 4. Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "903a45f6-1f93-41a6-860e-812a842482bc",
      "metadata": {
        "id": "903a45f6-1f93-41a6-860e-812a842482bc"
      },
      "source": [
        "Let's now define our model.\n",
        "\n",
        "There are several ways to create a model in Keras. This one is called the `\"Sequential API.\"`\n",
        "\n",
        "Our model will be a sequence of layers we will define individually."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b686d325-aa97-46b8-89c2-5f245acb92f1",
      "metadata": {
        "id": "b686d325-aa97-46b8-89c2-5f245acb92f1"
      },
      "source": [
        "A lot is going on with this first line.\n",
        "\n",
        "First, we define our model's input shape: a 28x28x1 tensor (width, height, channels.)\n",
        "\n",
        "This is exactly the shape we have in our train dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de2e4c18-b5fb-4aa8-940c-d6ac1c7f74df",
      "metadata": {
        "id": "de2e4c18-b5fb-4aa8-940c-d6ac1c7f74df"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f237a151-f410-4fbd-a87c-63f3e44b76f0",
      "metadata": {
        "id": "f237a151-f410-4fbd-a87c-63f3e44b76f0"
      },
      "source": [
        "A lot is going on with this first line.\n",
        "First, we define our model's input shape: a 28x28x1 tensor (width, height, channels.)\n",
        "This is exactly the shape we have in our train dataset.\n",
        "\n",
        "\n",
        "Then we define our first layer: a `Conv2D` layer with 32 filters and a 3x3 kernel.\n",
        "This layer will generate 32 different representations using the training images.\n",
        "\n",
        "\n",
        "We must also define the activation function used for this layer: `ReLU`.\n",
        "You'll see `ReLU` everywhere. It's a popular activation function.\n",
        "It will allow us to solve non-linear problems, like recognizing handwritten digits.ers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62f44d24-e3bf-4755-b6af-95221aec79a4",
      "metadata": {
        "id": "62f44d24-e3bf-4755-b6af-95221aec79a4"
      },
      "source": [
        "After our `Conv2D` layer, we have a max pooling operation.\n",
        "The goal of this layer is to downsample the amount of information collected by the convolutional layer.\n",
        "We want to throw away unimportant details and retain what truly matters..\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04d46117-b32c-4ff5-a360-a9188fae4dda",
      "metadata": {
        "id": "04d46117-b32c-4ff5-a360-a9188fae4dda"
      },
      "source": [
        "We are now going to flatten the output. We want everything in a continuous list of values.\n",
        "That is  what the `Flatten` layer does. It will give us a flat tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36da3838-4a59-4807-917b-f853a72bdab2",
      "metadata": {
        "id": "36da3838-4a59-4807-917b-f853a72bdab2"
      },
      "source": [
        "Finally, we have a couple of `Dense` layers.\n",
        "Notice how the output layer has a size of 10, one for each of our possible digit values, and a `softmax activation.`\n",
        "The `softmax` ensures we get a probability distribution indicating the most likely digit in the image."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7886722d-07e3-4a58-b569-cde0b296098d",
      "metadata": {
        "id": "7886722d-07e3-4a58-b569-cde0b296098d"
      },
      "source": [
        "## 4.1. Model compilation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3546f28-1af9-4615-b8ee-4a781478b911",
      "metadata": {
        "id": "e3546f28-1af9-4615-b8ee-4a781478b911"
      },
      "source": [
        "After creating our model, we compile it.\n",
        "I'm using `Stochastic Gradient Descent (SGD)` as the optimizer.\n",
        "The loss is categorical cross-entropy because this is a multi-class classification problem.\n",
        "We want to record the accuracy as the model trains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "766c19b2-9c6d-4a7c-befd-b66bbeaf3030",
      "metadata": {
        "id": "766c19b2-9c6d-4a7c-befd-b66bbeaf3030"
      },
      "outputs": [],
      "source": [
        "optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d08b3004-d02b-44cb-a423-8a129f305c30",
      "metadata": {
        "id": "d08b3004-d02b-44cb-a423-8a129f305c30"
      },
      "source": [
        "## 4.2. Fit the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b53eee2-ea44-4a98-8428-9cd15e901c33",
      "metadata": {
        "id": "9b53eee2-ea44-4a98-8428-9cd15e901c33"
      },
      "source": [
        "Finally, we fit the model. This starts the training process.\n",
        "\n",
        "A couple of notes:\n",
        "\n",
        "• I'm using a batch size of 32 images.\n",
        "• I'm running 10 total epochs.\n",
        "\n",
        "When fit() is done, we'll have a fully trained model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1293fe2d-8009-4828-9a34-a5c4976e165a",
      "metadata": {
        "id": "1293fe2d-8009-4828-9a34-a5c4976e165a",
        "outputId": "502e7f3c-bec4-4688-cbb0-e02dbb15bc8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 41s 19ms/step - loss: 0.2374 - accuracy: 0.9276\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0776 - accuracy: 0.9761\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 33s 17ms/step - loss: 0.0497 - accuracy: 0.9848\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 0.0362 - accuracy: 0.9888\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0267 - accuracy: 0.9916\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0200 - accuracy: 0.9940\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0149 - accuracy: 0.9956\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0112 - accuracy: 0.9967\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0087 - accuracy: 0.9976\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0059 - accuracy: 0.9985\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "760c1223-47d3-4de3-af18-5cb230ffc1bb",
      "metadata": {
        "id": "760c1223-47d3-4de3-af18-5cb230ffc1bb"
      },
      "source": [
        "## 4.3. Test the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7865f89a-0f9b-4936-ad3f-e918e6b9f28c",
      "metadata": {
        "id": "7865f89a-0f9b-4936-ad3f-e918e6b9f28c"
      },
      "source": [
        "Let's now test the model.\n",
        "\n",
        "This gets a random image from the test set and displays it.\n",
        "\n",
        "Notice that we want the image to come from the test set, containing data the model didn't see during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27d783d4-826c-4435-bda5-659c45501608",
      "metadata": {
        "id": "27d783d4-826c-4435-bda5-659c45501608",
        "outputId": "99450754-68b8-45fa-8197-d904907a45e8"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "image = random.choice(x_test)\n",
        "\n",
        "plt.imshow(image, cmap=plt.get_cmap('gray'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a29ea9d-5255-434b-aa13-34ac751dc387",
      "metadata": {
        "id": "2a29ea9d-5255-434b-aa13-34ac751dc387"
      },
      "source": [
        "Let's reshape and normalize the `image` as did to `x_train`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c2b88e2-15d9-4739-af76-3d6d33a48008",
      "metadata": {
        "id": "0c2b88e2-15d9-4739-af76-3d6d33a48008"
      },
      "outputs": [],
      "source": [
        "image = (image.reshape((1, 28, 28, 1))).astype('float32') / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96daffcb-d104-4468-9f88-df9cef1769dc",
      "metadata": {
        "id": "96daffcb-d104-4468-9f88-df9cef1769dc"
      },
      "source": [
        "Finally, I predict the value of the image.\n",
        "\n",
        "Remember that the result is a one-hot-encoded vector. That's why I take the argmax value (the position with the highest probability), and that's the result."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dab193e-4fa6-4598-b4c4-5e2d56b33d8e",
      "metadata": {
        "id": "4dab193e-4fa6-4598-b4c4-5e2d56b33d8e"
      },
      "source": [
        "# 5. Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e26c60dc-958b-4228-a786-8c7201bcb044",
      "metadata": {
        "id": "e26c60dc-958b-4228-a786-8c7201bcb044",
        "outputId": "4f5de9f4-b978-4ee7-a618-90d3e3b53785"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 299ms/step\n",
            "Prediction: 4\n"
          ]
        }
      ],
      "source": [
        "digit = np.argmax(model.predict(image)[0], axis=-1)\n",
        "print(\"Prediction:\", digit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fe718f8-33dd-4034-b437-dfc80655395c",
      "metadata": {
        "id": "8fe718f8-33dd-4034-b437-dfc80655395c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}